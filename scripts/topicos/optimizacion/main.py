# üîß CONFIGURACI√ìN INICIAL: Solucionar warnings de HuggingFace Tokenizers
import os
import warnings

# ===== SOLUCIONAR WARNINGS DE TOKENIZERS_PARALLELISM =====
# Estos warnings aparecen cuando HuggingFace Tokenizers usa paralelismo y luego 
# se crean nuevos procesos (como en optimizaci√≥n con Optuna)
os.environ["TOKENIZERS_PARALLELISM"] = "false"
print("‚úì TOKENIZERS_PARALLELISM configurado a 'false' para evitar warnings")

# Opcional: Suprimir otros warnings comunes
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning, module="transformers")

print("‚úì Configuraci√≥n de warnings completada")
print("=" * 50)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from umap import UMAP
from hdbscan import HDBSCAN
from sklearn.feature_extraction.text import CountVectorizer
import os
import sys
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import PromptTemplate
from pydantic import BaseModel, Field
from typing import Literal
import nltk
import optuna
from sklearn.metrics import silhouette_score
from sklearn.feature_extraction.text import TfidfVectorizer
import warnings
optuna.logging.set_verbosity(optuna.logging.WARNING)  # Reducir logs verbosos

import nltk
from nltk.tokenize import word_tokenize
from gensim.models.coherencemodel import CoherenceModel
from gensim.corpora import Dictionary
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import MinMaxScaler

nltk.download('punkt_tab')

# Agregar la carpeta scripts al path para importar m√≥dulos
# Obtener el directorio ra√≠z del proyecto (3 niveles arriba desde este archivo)
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
scripts_path = os.path.join(project_root, 'scripts')
sys.path.insert(0, scripts_path)

# Importar m√≥dulo de limpieza de texto
from topicos.limpieza_texto_mejorado import LimpiadorTextoMejorado
from topicos.utils_topicos import generar_reporte_limpieza, mostrar_ejemplos_limpieza

# Importar funciones de optimizaci√≥n
from topicos.optimizacion.caracteristicas_ciudad import analizar_caracteristicas_dataset, analizar_caracteristicas_multiciudad
from topicos.optimizacion.metricas import calcular_coherencia_topicos, normalizar_metricas
from topicos.optimizacion.objetivo import calcular_pesos_dinamicos, calcular_puntuacion_objetivo, objetivo_bertopic_multiciudad

load_dotenv()
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

class TopicNaming(BaseModel):
    nombre_topico: str = Field(description="Nombre descriptivo del t√≥pico en espa√±ol")

def configurar_clasificador_topicos():
    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0,
        max_tokens=50
    )
    
    parser = PydanticOutputParser(pydantic_object=TopicNaming)
    
    prompt_template = """Eres un experto en an√°lisis de opiniones tur√≠sticas y modelado de t√≥picos.

Analiza las siguientes palabras clave que representan un t√≥pico identificado autom√°ticamente en rese√±as de atracciones tur√≠sticas en M√©xico (Canc√∫n, CDMX, Mazatl√°n, Puebla, Puerto Vallarta).

Palabras clave del t√≥pico: {keywords}

Bas√°ndote en estas palabras, asigna un nombre descriptivo y coherente al t√≥pico que capture la esencia de las opiniones tur√≠sticas que representa. El nombre debe ser:
- Espec√≠fico y relacionado con turismo
- En espa√±ol
- M√°ximo 4 palabras
- Descriptivo de la experiencia o aspecto tur√≠stico
- Evitar mencionar entidades espec√≠ficas (nombres de lugares, marcas, personas)

{format_instructions}
"""
    
    prompt = PromptTemplate(
        template=prompt_template,
        input_variables=["keywords"],
        partial_variables={"format_instructions": parser.get_format_instructions()}
    )
    
    return prompt | llm | parser

clasificador_topicos = configurar_clasificador_topicos()

# Configurar ciudad a analizar
CIUDAD_ANALIZAR = "Cdmx"  # Cambiar por la ciudad deseada: Cancun, Cdmx, Mazatlan, Puebla, Puerto_Vallarta

# üÜï NUEVA VARIABLE: Ciudades para optimizaci√≥n de hiperpar√°metros (cross-validation entre datasets)
CIUDADES_OPTIMIZACION = ["Cancun", "Cdmx", "Mazatlan", "Puebla", "Puerto_vallarta"]

df = pd.read_csv('data/processed/dataset_opiniones_analisis.csv')

# Inicializar columna TopicoConBERTopic si no existe
if 'TopicoConBERTopic' not in df.columns:
    df['TopicoConBERTopic'] = np.nan

print(f"Dataset cargado: {df.shape[0]} opiniones")
print(f"Columnas disponibles: {list(df.columns)}")
print(f"\nDistribuci√≥n total por ciudad:")
print(df['Ciudad'].value_counts())

# Validar ciudades disponibles para optimizaci√≥n
ciudades_disponibles = df['Ciudad'].unique()
ciudades_optimizacion_validas = [c for c in CIUDADES_OPTIMIZACION if c in ciudades_disponibles]
print(f"\nüéØ Ciudades configuradas para optimizaci√≥n: {CIUDADES_OPTIMIZACION}")
print(f"‚úÖ Ciudades disponibles en el dataset: {ciudades_optimizacion_validas}")

if len(ciudades_optimizacion_validas) != len(CIUDADES_OPTIMIZACION):
    ciudades_faltantes = set(CIUDADES_OPTIMIZACION) - set(ciudades_optimizacion_validas)
    print(f"‚ö†Ô∏è Ciudades no encontradas en el dataset: {list(ciudades_faltantes)}")

# Filtrar por ciudad espec√≠fica para an√°lisis individual
df_ciudad = df[df['Ciudad'] == CIUDAD_ANALIZAR].copy()
print(f"\nüéØ Analizando ciudad individual: {CIUDAD_ANALIZAR}")
print(f"Opiniones de {CIUDAD_ANALIZAR}: {len(df_ciudad)}")

# Siempre usar texto limpio para BERTopic si est√° disponible
columna_texto = 'TituloReviewLimpio'
print(f"üìù Usando texto limpio para an√°lisis de t√≥picos")

texts = df_ciudad[columna_texto].dropna().tolist()

print(f"üìù Columna de texto utilizada: {columna_texto}")
print(f"üßπ Textos para an√°lisis: {len(texts)} opiniones")

# Mostrar estad√≠sticas de texto
if len(texts) > 0:
    palabras_promedio = sum(len(text.split()) for text in texts) / len(texts)
    print(f"üìä Promedio de palabras por texto: {palabras_promedio:.1f}")
    print(f"üìè Longitud promedio: {sum(len(text) for text in texts) / len(texts):.1f} caracteres")

# üìä Estad√≠sticas para optimizaci√≥n multi-ciudad
print(f"\nüìà ESTAD√çSTICAS PARA OPTIMIZACI√ìN MULTI-CIUDAD:")
print(f"=" * 60)
for ciudad in ciudades_optimizacion_validas:
    df_temp = df[df['Ciudad'] == ciudad]
    textos_temp = df_temp[columna_texto].dropna()
    print(f"üèôÔ∏è {ciudad}: {len(textos_temp)} opiniones v√°lidas")
    if len(textos_temp) > 0:
        palabras_prom = textos_temp.str.split().str.len().mean()
        print(f"   üìù Promedio palabras: {palabras_prom:.1f}")
print(f"=" * 60)

# ===== LIMPIEZA DE TEXTO PARA AN√ÅLISIS DE T√ìPICOS =====
print("üßπ Iniciando proceso de limpieza de texto...")

# Crear instancia del limpiador
limpiador = LimpiadorTextoMejorado(idiomas=['spanish', 'english'])

# Guardar DataFrame antes de la limpieza para comparaci√≥n
df_antes = df.copy()

if 'TituloReviewLimpio' not in df.columns:
    # Aplicar limpieza al dataset completo
    df = limpiador.limpiar_dataframe(
        df, 
        columna_texto='TituloReview',
        nombre_columna_limpia='TituloReviewLimpio',
        aplicar_lematizacion=True,
        min_longitud_palabra=2,
        max_palabras=None
    )

# Mostrar ejemplos de limpieza
print(f"\nüîç Ejemplos de limpieza aplicada:")
mostrar_ejemplos_limpieza(df, n_ejemplos=3)

# Generar reporte completo
generar_reporte_limpieza(df_antes, df, 'TituloReview', 'TituloReviewLimpio')

# Guardar dataset actualizado
df.to_csv('data/processed/dataset_opiniones_analisis.csv', index=False)
print(f"\nüíæ Dataset actualizado y guardado con columna 'TituloReviewLimpio'")


print(f"\nüìã Estructura actual del dataset:")
print(f"Dimensiones: {df.shape}")
print(f"Columnas: {list(df.columns)}")

# Mostrar posici√≥n de las columnas de texto
pos_original = df.columns.get_loc('TituloReview') + 1
pos_limpia = df.columns.get_loc('TituloReviewLimpio') + 1
print(f"üìç TituloReview: columna {pos_original}")
print(f"üìç TituloReviewLimpio: columna {pos_limpia}")

# ===== AN√ÅLISIS DEL DATASET PARA OPTIMIZACI√ìN =====

def analizar_caracteristicas_dataset(texts):
    """Analiza las caracter√≠sticas del dataset para guiar la optimizaci√≥n"""
    
    print("üîç Analizando caracter√≠sticas del dataset...")
    
    # Estad√≠sticas b√°sicas
    num_docs = len(texts)
    avg_length = np.mean([len(text.split()) for text in texts])
    std_length = np.std([len(text.split()) for text in texts])
    min_length = min([len(text.split()) for text in texts])
    max_length = max([len(text.split()) for text in texts])
    
    # Diversidad l√©xica
    all_words = set()
    for text in texts:
        all_words.update(text.split())
    vocab_size = len(all_words)
    
    # An√°lisis de duplicados
    unique_texts = len(set(texts))
    duplicate_ratio = 1 - (unique_texts / num_docs)
    
    characteristics = {
        'num_docs': num_docs,
        'avg_length': avg_length,
        'std_length': std_length,
        'min_length': min_length,
        'max_length': max_length,
        'vocab_size': vocab_size,
        'unique_texts': unique_texts,
        'duplicate_ratio': duplicate_ratio,
        'lexical_diversity': vocab_size / sum(len(text.split()) for text in texts)
    }
    
    print(f"üìä Caracter√≠sticas del dataset:")
    print(f"   ‚Ä¢ N√∫mero de documentos: {num_docs}")
    print(f"   ‚Ä¢ Longitud promedio: {avg_length:.1f} ¬± {std_length:.1f} palabras")
    print(f"   ‚Ä¢ Rango de longitud: {min_length} - {max_length} palabras")
    print(f"   ‚Ä¢ Vocabulario √∫nico: {vocab_size:,} palabras")
    print(f"   ‚Ä¢ Textos √∫nicos: {unique_texts} ({(1-duplicate_ratio)*100:.1f}%)")
    print(f"   ‚Ä¢ Diversidad l√©xica: {characteristics['lexical_diversity']:.4f}")
    
    return characteristics

def analizar_caracteristicas_multiciudad(df, ciudades, columna_texto):
    """
    üÜï Analiza las caracter√≠sticas de m√∫ltiples ciudades para optimizaci√≥n cross-validation
    """
    print("üåç Analizando caracter√≠sticas multi-ciudad para optimizaci√≥n...")
    
    caracteristicas_por_ciudad = {}
    
    for ciudad in ciudades:
        print(f"\nüèôÔ∏è Analizando {ciudad}:")
        df_ciudad = df[df['Ciudad'] == ciudad]
        texts_ciudad = df_ciudad[columna_texto].dropna().tolist()
    
        caracteristicas_por_ciudad[ciudad] = analizar_caracteristicas_dataset(texts_ciudad)
    
    # Calcular estad√≠sticas combinadas
    total_docs = sum(char['num_docs'] for char in caracteristicas_por_ciudad.values() if char is not None)
    ciudades_validas = [ciudad for ciudad, char in caracteristicas_por_ciudad.items() if char is not None]
    
    if ciudades_validas:
        avg_docs_per_city = total_docs / len(ciudades_validas)
        min_docs = min(char['num_docs'] for char in caracteristicas_por_ciudad.values() if char is not None)
        max_docs = max(char['num_docs'] for char in caracteristicas_por_ciudad.values() if char is not None)
        
        # Diversidad l√©xica promedio
        avg_lexical_diversity = np.mean([char['lexical_diversity'] for char in caracteristicas_por_ciudad.values() if char is not None])
        
        caracteristicas_combinadas = {
            'total_docs': total_docs,
            'num_ciudades': len(ciudades_validas),
            'avg_docs_per_city': avg_docs_per_city,
            'min_docs_city': min_docs,
            'max_docs_city': max_docs,
            'avg_lexical_diversity': avg_lexical_diversity,
            'ciudades_validas': ciudades_validas,
            'por_ciudad': caracteristicas_por_ciudad
        }
        
        print(f"\nüìà RESUMEN MULTI-CIUDAD:")
        print(f"   ‚Ä¢ Total documentos: {total_docs:,}")
        print(f"   ‚Ä¢ Ciudades v√°lidas: {len(ciudades_validas)}")
        print(f"   ‚Ä¢ Promedio docs/ciudad: {avg_docs_per_city:.1f}")
        print(f"   ‚Ä¢ Rango docs/ciudad: {min_docs} - {max_docs}")
        print(f"   ‚Ä¢ Diversidad l√©xica promedio: {avg_lexical_diversity:.4f}")
        
        return caracteristicas_combinadas
    else:
        print("‚ùå No se encontraron ciudades v√°lidas")
        return None

# üÜï Analizar caracter√≠sticas multi-ciudad para optimizaci√≥n
if len(ciudades_optimizacion_validas) > 1:
    print(f"\n" + "="*80)
    dataset_chars_multiciudad = analizar_caracteristicas_multiciudad(
        df, ciudades_optimizacion_validas, columna_texto
    )
    print(f"="*80)
else:
    print(f"\n‚ö†Ô∏è Solo hay una ciudad v√°lida para optimizaci√≥n multi-ciudad")
    dataset_chars_multiciudad = None

print("üîß Funciones objetivo cargadas")

# ===== EJECUTAR OPTIMIZACI√ìN CON OPTUNA =====

# Configuraci√≥n de la optimizaci√≥n
OPTIMIZATION_CONFIG = {
    'n_trials': 100,  # N√∫mero de pruebas (ajustar seg√∫n tiempo disponible)
    'timeout': 360000,  # Timeout en segundos (1 hora)
    'n_jobs': 1  # Paralelizaci√≥n (cuidado con memoria)
}

if dataset_chars_multiciudad is not None:
    print(f"üåç OPTIMIZACI√ìN MULTI-CIUDAD ACTIVADA")
    print(f"üéØ Cross-validation entre {len(ciudades_optimizacion_validas)} ciudades")
    print(f"‚úÖ Ciudades: {ciudades_optimizacion_validas}")
    print(f"üìä Total documentos: {dataset_chars_multiciudad['total_docs']:,}")
    print(f"üìà Promedio docs/ciudad: {dataset_chars_multiciudad['avg_docs_per_city']:.1f}")
    
    OPTIMIZATION_CONFIG['sample_size'] = dataset_chars_multiciudad['total_docs']
    optimization_function = lambda trial: objetivo_bertopic_multiciudad(
        trial, df, ciudades_optimizacion_validas, columna_texto, dataset_chars_multiciudad
    )    
    
print(f"\nüìä Configuraci√≥n de optimizaci√≥n:")
print(f"   ‚Ä¢ Muestra para optimizaci√≥n: {OPTIMIZATION_CONFIG['sample_size']:,} textos")
print(f"   ‚Ä¢ N√∫mero de pruebas: {OPTIMIZATION_CONFIG['n_trials']}")

def save_callback(study, trial):
    df = study.trials_dataframe(attrs=("number", "value", "params", "state"))
    df.to_csv("optuna_trials.csv", index=False)

# Crear estudio de Optuna
study_name = f"bertopic_multiciudad"
study = optuna.create_study(
    direction='maximize',  # Maximizar la puntuaci√≥n objetivo
    sampler=optuna.samplers.TPESampler(seed=42),
    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=5),
    study_name=study_name,
    storage="sqlite:///resultados.db",
    load_if_exists=True
)

print(f"\nüîß Iniciando optimizaci√≥n ({study_name})...")

# Ejecutar optimizaci√≥n
start_time = pd.Timestamp.now()


study.optimize(
    optimization_function,
    n_trials=OPTIMIZATION_CONFIG['n_trials'],
    timeout=OPTIMIZATION_CONFIG['timeout'],
    n_jobs=OPTIMIZATION_CONFIG['n_jobs'],
    callbacks=[save_callback]
)

optimization_time = pd.Timestamp.now() - start_time
print(f"\n‚úÖ Optimizaci√≥n completada en {optimization_time}")
    
print(f"\nüìà RESULTADO DE LA OPTIMIZACI√ìN MULTI-CIUDAD:")
print(f"‚úÖ Los hiperpar√°metros encontrados funcionan bien en:")
for ciudad in ciudades_optimizacion_validas:
    docs_ciudad = dataset_chars_multiciudad['por_ciudad'][ciudad]['num_docs']
    print(f"   üèôÔ∏è {ciudad}: {docs_ciudad} documentos")
