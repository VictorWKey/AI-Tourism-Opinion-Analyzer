"""
Utilidades de Procesamiento - Funciones auxiliares y pipeline principal
"""

import pandas as pd
import os
from .cargador_datos import CargadorDatos
from .limpieza_datos import LimpiadorDatos
from .transformador_datos import TransformadorDatos
from .validador_datos import ValidadorDatos


def verificar_datasets_creados(directorio_ciudades, df_original):
    """
    Verifica los datasets por ciudad creados y genera estad√≠sticas.
    
    Args:
        directorio_ciudades (str): Directorio donde est√°n los datasets por ciudad
        df_original (DataFrame): Dataset original para comparar
        
    Returns:
        list: Lista de diccionarios con informaci√≥n de cada dataset creado
    """
    if not os.path.exists(directorio_ciudades):
        print(f"‚ùå Directorio no encontrado: {directorio_ciudades}")
        return []
    
    archivos_creados = [f for f in os.listdir(directorio_ciudades) if f.endswith('.csv')]
    ciudades_reales = df_original['Ciudad'].unique()
    datasets_creados = []
    
    print(f"\nüìä RESUMEN:")
    print(f"   ‚Ä¢ Archivos encontrados: {len(archivos_creados)}")
    print(f"   ‚Ä¢ Total de filas en dataset principal: {len(df_original):,}")
    
    # Filtrar y procesar solo archivos de ciudades reales
    for archivo in archivos_creados:
        if archivo.startswith('dataset_') and archivo.endswith('.csv'):
            nombre_ciudad = archivo.replace('dataset_', '').replace('.csv', '')
            
            # Verificar que corresponda a una ciudad real
            if nombre_ciudad in [ciudad.lower() for ciudad in ciudades_reales]:
                ruta_archivo = os.path.join(directorio_ciudades, archivo)
                tama√±o_kb = os.path.getsize(ruta_archivo) / 1024
                ciudad_nombre = nombre_ciudad.title()
                
                datasets_creados.append({
                    'ciudad': ciudad_nombre,
                    'archivo': archivo,
                    'ruta': ruta_archivo,
                    'tama√±o_kb': tama√±o_kb
                })
    
    print(f"   ‚Ä¢ Archivos de ciudades v√°lidos: {len(datasets_creados)}")
    
    print(f"\nüìã ARCHIVOS DE CIUDADES GENERADOS:")
    for dataset in sorted(datasets_creados, key=lambda x: x['ciudad']):
        print(f"   üìÑ {dataset['ciudad']}: {dataset['archivo']} ({dataset['tama√±o_kb']:.1f} KB)")
    
    return datasets_creados


def mostrar_resumen_detallado_datasets(datasets_creados, df_original):
    """
    Muestra un resumen detallado de los datasets por ciudad creados.
    VERSI√ìN OPTIMIZADA - Sin lecturas innecesarias de archivos.
    
    Args:
        datasets_creados (list): Lista de datasets creados
        df_original (DataFrame): Dataset original para comparar
    """
    if not datasets_creados:
        print("‚ö†Ô∏è No hay datasets de ciudades para mostrar")
        return
    
    print("=== RESUMEN DETALLADO DE DATASETS POR CIUDAD ===")
    
    print(f"\nüìä Estad√≠sticas generales:")
    print(f"   ‚Ä¢ Total de ciudades: {len(datasets_creados)}")
    print(f"   ‚Ä¢ Total de filas en dataset principal: {len(df_original):,}")
    print(f"   ‚Ä¢ Ciudades reales: {', '.join(df_original['Ciudad'].unique())}")
    
    # Obtener estad√≠sticas del dataset original (m√°s eficiente)
    print(f"\nüìã Detalle por ciudad:")
    total_archivos = 0
    total_filas_ciudades = 0
    
    # Calcular estad√≠sticas desde el dataset original (evita leer archivos)
    distribuci√≥n_ciudades = df_original['Ciudad'].value_counts()
    
    for i, dataset in enumerate(datasets_creados, 1):
        if os.path.exists(dataset['ruta']):
            # Obtener filas desde el conteo del dataset original (m√°s eficiente)
            ciudad_original = None
            for ciudad in distribuci√≥n_ciudades.index:
                if ciudad.lower() == dataset['ciudad'].lower():
                    ciudad_original = ciudad
                    break
            
            if ciudad_original:
                filas = distribuci√≥n_ciudades[ciudad_original]
                columnas = len(df_original.columns)  # Mismo n√∫mero de columnas
                
                porcentaje = (filas / len(df_original)) * 100
                total_filas_ciudades += filas
                
                print(f"   {i:2d}. {dataset['ciudad']:15} ‚Üí {dataset['archivo']:25}")
                print(f"       üìä {filas:,} filas ({porcentaje:5.1f}%) | {columnas} columnas | {dataset['tama√±o_kb']:.1f} KB")
                total_archivos += 1
            else:
                print(f"   {i:2d}. {dataset['ciudad']:15} ‚Üí ‚ö†Ô∏è Ciudad no encontrada en dataset")
        else:
            print(f"   {i:2d}. {dataset['ciudad']:15} ‚Üí ‚ùå Archivo no encontrado")
    
    # Verificaci√≥n de integridad
    print(f"\nüîç VERIFICACI√ìN DE INTEGRIDAD:")
    print(f"   ‚Ä¢ Archivos creados correctamente: {total_archivos}/{len(datasets_creados)}")
    print(f"   ‚Ä¢ Total filas calculadas: {total_filas_ciudades:,}")
    print(f"   ‚Ä¢ Total filas en dataset principal: {len(df_original):,}")
    
    if total_filas_ciudades == len(df_original):
        print(f"   ‚úÖ Integridad verificada: todas las filas est√°n presentes")
    else:
        print(f"   ‚ö†Ô∏è  Advertencia: discrepancia en el n√∫mero de filas")
    
    # Mostrar columnas del dataset original (evita leer archivo)
    print(f"\nüìã Columnas disponibles en datasets por ciudad:")
    for i, col in enumerate(df_original.columns, 1):
        print(f"   {i:2d}. {col}")
    
    # Mostrar muestra r√°pida solo del primer dataset (lectura m√≠nima)
    if datasets_creados and os.path.exists(datasets_creados[0]['ruta']):
        dataset_ejemplo = datasets_creados[0]
        print(f"\nüìñ Muestra del dataset '{dataset_ejemplo['archivo']}':")
        
        # Usar el dataset original filtrado (m√°s eficiente que leer archivo)
        ciudad_filtro = dataset_ejemplo['ciudad']
        df_muestra = df_original[df_original['Ciudad'].str.lower() == ciudad_filtro.lower()].head(3)
        print("Primeras 3 filas:")
        print(df_muestra.to_string())
    
    print(f"\nüéØ DATASETS LISTOS PARA:")
    print(f"   ‚Ä¢ An√°lisis de sentimientos por ciudad")
    print(f"   ‚Ä¢ Comparaciones entre ciudades") 
    print(f"   ‚Ä¢ Modelos de machine learning espec√≠ficos")
    print(f"   ‚Ä¢ Visualizaciones personalizadas")
    
    print(f"\nüìÅ ESTRUCTURA FINAL:")
    print(f"   data/processed/datasets_por_ciudad/")
    for dataset in datasets_creados:
        print(f"   ‚îú‚îÄ‚îÄ {dataset['archivo']}")
    
    print(f"\nüéâ ¬°{len(datasets_creados)} datasets de ciudades listos para an√°lisis!")


def resumen_rapido_datasets(directorio_ciudades, df_original):
    """
    Funci√≥n ultra-r√°pida para mostrar resumen b√°sico sin lecturas de archivos.
    
    Args:
        directorio_ciudades (str): Directorio donde est√°n los datasets
        df_original (DataFrame): Dataset original
        
    Returns:
        dict: Resumen b√°sico de los datasets
    """
    if not os.path.exists(directorio_ciudades):
        return {"error": "Directorio no encontrado"}
    
    archivos = [f for f in os.listdir(directorio_ciudades) if f.endswith('.csv')]
    ciudades_en_datos = df_original['Ciudad'].unique()
    
    resumen = {
        "total_archivos": len(archivos),
        "ciudades_en_datos": len(ciudades_en_datos),
        "total_filas": len(df_original),
        "distribuci√≥n": df_original['Ciudad'].value_counts().to_dict(),
        "archivos_encontrados": archivos
    }
    
    print(f"üìä RESUMEN R√ÅPIDO:")
    print(f"   ‚Ä¢ Archivos CSV encontrados: {len(archivos)}")
    print(f"   ‚Ä¢ Ciudades en dataset: {len(ciudades_en_datos)}")
    print(f"   ‚Ä¢ Total de opiniones: {len(df_original):,}")
    
    print(f"\nüìã Distribuci√≥n por ciudad:")
    for ciudad, cantidad in df_original['Ciudad'].value_counts().items():
        porcentaje = (cantidad / len(df_original)) * 100
        print(f"   ‚Ä¢ {ciudad}: {cantidad:,} ({porcentaje:.1f}%)")
    
    return resumen


def obtener_rutas_data(ruta_base='../data'):
    """
    Obtiene las rutas de los directorios raw y processed.
    
    Args:
        ruta_base (str): Ruta base del directorio data
        
    Returns:
        tuple: (ruta_raw, ruta_processed)
    """
    ruta_raw = os.path.join(ruta_base, 'raw')
    ruta_processed = os.path.join(ruta_base, 'processed')
    return ruta_raw, ruta_processed


def crear_directorios_data(ruta_base='../data'):
    """
    Crea los directorios raw y processed si no existen.
    
    Args:
        ruta_base (str): Ruta base del directorio data
    """
    ruta_raw, ruta_processed = obtener_rutas_data(ruta_base)
    os.makedirs(ruta_raw, exist_ok=True)
    os.makedirs(ruta_processed, exist_ok=True)
    print(f"‚úÖ Directorios verificados: {ruta_raw}, {ruta_processed}")


def capitalizar_palabras(texto):
    """
    Transforma a may√∫scula la primera letra de **todas** las palabras de un texto.
    
    Args:
        texto (str): El texto a transformar
        
    Returns:
        str: El texto con la primera letra de cada palabra en may√∫scula
    """
    if not texto or not isinstance(texto, str):
        return texto
    
    # Limpiar espacios al inicio y final
    texto = texto.strip()
    
    if len(texto) == 0:
        return texto
    
    # Capitalizar cada palabra
    palabras = texto.split()
    palabras_capitalizadas = [palabra[0].upper() + palabra[1:] if palabra else '' for palabra in palabras]
    return ' '.join(palabras_capitalizadas)


def extraer_nombre_atraccion(nombre_archivo):
    """
    Extrae el nombre de la atracci√≥n del nombre del archivo CSV.
    Por ejemplo: 'cancun-la-isla.csv' -> 'la isla'
    """
    # Remover la extensi√≥n .csv
    nombre_sin_extension = nombre_archivo.replace('.csv', '')
    
    # Dividir por guiones
    partes = nombre_sin_extension.split('-')
    
    # Omitir la primera parte (que es el nombre de la ciudad)
    # y unir el resto con espacios
    if len(partes) > 1:
        atraccion = ' '.join(partes[1:])
    else:
        atraccion = nombre_sin_extension
    
    return atraccion


def procesar_dataset_completo(ruta_data='../data'):
    """
    Pipeline completo de procesamiento del dataset.
    """
    print("="*80)
    print("             PIPELINE DE PROCESAMIENTO COMPLETO")
    print("="*80)
    
    # Crear directorios si no existen
    crear_directorios_data(ruta_data)
    
    # 1. Cargar datos
    print("\nüîÑ PASO 1: Cargando datos...")
    cargador = CargadorDatos(ruta_data)
    df = cargador.cargar_datos_turisticos()
    
    if df.empty:
        print("‚ùå Error: No se pudieron cargar los datos")
        return pd.DataFrame()
    
    # 2. Convertir fechas
    print("\nüîÑ PASO 2: Convirtiendo fechas...")
    transformador = TransformadorDatos(df)
    df = transformador.convertir_fechas()
    
    # 3. Limpieza espec√≠fica de fechas - NUEVO
    print("\nüîÑ PASO 3: Limpieza espec√≠fica de columnas de fechas...")
    limpiador = LimpiadorDatos(df)
    
    # 3a. Eliminar columna FechaOpinion por exceso de nulos
    df = limpiador.eliminar_columna_fechaopinion()
    
    # 3b. Eliminar filas con FechaEstadia nula
    df = limpiador.eliminar_filas_fechaestadia_nulas()
    
    # 4. Limpiar columna OrigenAutor
    print("\nüîÑ PASO 4: Limpiando columna OrigenAutor...")
    df = limpiador.limpiar_columna_origen_autor()
    
    # 5. Completar valores nulos
    print("\nüîÑ PASO 5: Completando valores nulos...")
    df = limpiador.completar_valores_nulos()
    
    # 6. Eliminar duplicados
    print("\nüîÑ PASO 6: Eliminando duplicados...")
    df = limpiador.eliminar_duplicados()
    
    # 7. Corregir contenidos mal ubicados
    print("\nüîÑ PASO 7: Corrigiendo contenidos mal ubicados...")
    validador = ValidadorDatos(df)
    df = validador.examinar_y_corregir_contenidos_mal_ubicados()
    
    # 8. Crear texto consolidado
    print("\nüîÑ PASO 8: Creando texto consolidado...")
    transformador = TransformadorDatos(df)
    df = transformador.agregar_texto_consolidado()
    
    # 9. Aplicar capitalizaci√≥n a columnas categ√≥ricas
    print("\nüîÑ PASO 9: Aplicando capitalizaci√≥n a columnas categ√≥ricas...")
    df['Ciudad'] = df['Ciudad'].apply(capitalizar_palabras)
    df['Atraccion'] = df['Atraccion'].apply(capitalizar_palabras)
    print(f"‚úÖ Capitalizaci√≥n aplicada a ciudades y atracciones")
    
    # 10. Guardar dataset final
    print("\nüîÑ PASO 10: Guardando dataset procesado...")
    validador = ValidadorDatos(df)
    df = validador.guardar_dataset_procesado()
    
    # 11. Exportar datasets por ciudad
    print("\nüîÑ PASO 11: Exportando datasets por ciudad...")
    exportar_datasets_por_ciudad(df, ruta_data)
    
    print("\n‚úÖ PIPELINE DE PROCESAMIENTO COMPLETADO EXITOSAMENTE")
    print(f"üìä Dataset final: {len(df)} filas, {len(df.columns)} columnas")
    print("\nüìÅ NUEVA ESTRUCTURA DE DATOS ORGANIZADA:")
    print(f"   üìã Datasets base: data/processed/datasets_por_ciudad/base/")
    print(f"   üí≠ Sentimientos: data/processed/datasets_por_ciudad/sentimientos/")
    print(f"   üéØ Subjetividad: data/processed/datasets_por_ciudad/subjetividad/")
    print(f"   üîó Combinados: data/processed/datasets_por_ciudad/combinado/")
    
    return df


def exportar_datasets_por_ciudad(df, ruta_data='../data'):
    """
    Exporta datasets separados por ciudad al directorio processed.
    
    Args:
        df (DataFrame): Dataset consolidado
        ruta_data (str): Ruta base del directorio data
    """
    ruta_raw, ruta_processed = obtener_rutas_data(ruta_data)
    directorio_ciudades = os.path.join(ruta_processed, 'datasets_por_ciudad')
    directorio_base = os.path.join(directorio_ciudades, 'base')
    os.makedirs(directorio_base, exist_ok=True)
    
    print(f"üìÅ Directorio creado/verificado: {directorio_base}")
    
    # Exportar por ciudad
    ciudades = df['Ciudad'].unique()
    for ciudad in ciudades:
        df_ciudad = df[df['Ciudad'] == ciudad]
        nombre_archivo = f"dataset_{ciudad.lower()}.csv"
        ruta_archivo = os.path.join(directorio_base, nombre_archivo)
        
        df_ciudad.to_csv(ruta_archivo, index=False)
        print(f"üíæ {nombre_archivo}: {len(df_ciudad)} filas guardadas")
        print(f"üìÅ Ubicaci√≥n: {ruta_archivo}")
    
    print(f"‚úÖ Exportaci√≥n por ciudades completada en: {directorio_ciudades}")
    print(f"ÔøΩ {len(ciudades)} datasets de ciudades creados")
    return directorio_ciudades
